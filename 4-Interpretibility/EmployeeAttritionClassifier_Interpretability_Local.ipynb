{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpretability With Tensorflow On Azure Machine Learning Service (Local)\n",
    "\n",
    "## Overview of Tutorial\n",
    "This notebook is Part 4 (Explaining Your Model Using Interpretability) of a four part workshop that demonstrates an end-to-end workflow for using Tensorflow on Azure Machine Learning Service. The different components of the workshop are as follows:\n",
    "\n",
    "- Part 1: [Preparing Data and Model Training](https://github.com/microsoft/bert-stack-overflow/blob/master/1-Training/AzureServiceClassifier_Training.ipynb)\n",
    "- Part 2: [Inferencing and Deploying a Model](https://github.com/microsoft/bert-stack-overflow/blob/master/2-Inferencing/AzureServiceClassifier_Inferencing.ipynb)\n",
    "- Part 3: [Setting Up a Pipeline Using MLOps](https://github.com/microsoft/bert-stack-overflow/tree/master/3-ML-Ops)\n",
    "- Part 4: [Explaining Your Model Interpretability](https://github.com/microsoft/bert-stack-overflow/blob/master/4-Interpretibility/IBMEmployeeAttritionClassifier_Interpretability.ipynb)\n",
    "\n",
    "_**This notebook showcases how to use the Azure Machine Learning Interpretability SDK to train and explain a binary classification model locally.**_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Azure Machine Learning Service?\n",
    "Azure Machine Learning service is a cloud service that you can use to develop and deploy machine learning models. Using Azure Machine Learning service, you can track your models as you build, train, deploy, and manage them, all at the broad scale that the cloud provides.\n",
    "![](./images/aml-overview.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What Is Machine Learning Interpretability?\n",
    "Interpretability is the ability to explain why your model made the predictions it did. The Azure Machine Learning service offers various interpretability features to help accomplish this task. These features include:\n",
    "\n",
    "- Feature importance values for both raw and engineered features.\n",
    "- Interpretability on real-world datasets at scale, during training and inference.\n",
    "- Interactive visualizations to aid you in the discovery of patterns in data and explanations at training time.\n",
    "\n",
    "By accurately interpretabiliting your model, it allows you to:\n",
    "\n",
    "- Use the insights for debugging your model.\n",
    "- Validate model behavior matches their objectives.\n",
    "- Check for for bias in the model.\n",
    "- Build trust in your customers and stakeholders.\n",
    "\n",
    "![](./images/interpretability-architecture.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Change Tensorflow and Interpret Library Versions\n",
    "\n",
    "We will be using an older version (1.14) for this particular tutorial in the series as Tensorflow 2.0 is not yet supported for Interpretibility on Azure Machine Learning service. We will also be using version 0.1.0.4 of the interpret library. \n",
    "\n",
    "If haven't already done so, please update your library versions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %pip uninstall tensorflow-gpu keras --yes\n",
    "# %pip install tensorflow-gpu==1.14 interpret-community==0.1.0.4\n",
    "%pip install interpret-community==0.18.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After installing packages, you must close and reopen the notebook as well as restarting the kernel.\n",
    "\n",
    "Let's make sure we have the right verisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import interpret_community\n",
    "\n",
    "print(tf.version.VERSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model\n",
    "For this tutorial, we will be using the *tf.keras module* to train a basic feed forward neural network on the IBM Employee Attrition Dataset. \n",
    "\n",
    "**We will start by writing the training script to train our model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def preprocess_data(data):\n",
    "    '''\n",
    "    \n",
    "    '''\n",
    "    # Dropping Employee count as all values are 1 and hence attrition is independent of this feature\n",
    "    data = data.drop(['EmployeeCount'], axis=1)\n",
    "    \n",
    "    # Dropping Employee Number since it is merely an identifier\n",
    "    data = data.drop(['EmployeeNumber'], axis=1)\n",
    "    data = data.drop(['Over18'], axis=1)\n",
    "\n",
    "    # Since all values are 80\n",
    "    data = data.drop(['StandardHours'], axis=1)\n",
    "\n",
    "    # Converting target variables from string to numerical values\n",
    "    target_map = {'Yes': 1, 'No': 0}\n",
    "    data[\"Attrition_numerical\"] = data[\"Attrition\"].apply(lambda x: target_map[x])\n",
    "    target = data[\"Attrition_numerical\"]\n",
    "\n",
    "    data.drop(['Attrition_numerical', 'Attrition'], axis=1, inplace=True)\n",
    "    \n",
    "    # Creating dummy columns for each categorical feature\n",
    "    categorical = []\n",
    "    for col, value in data.iteritems():\n",
    "        if value.dtype == 'object':\n",
    "            categorical.append(col)\n",
    "\n",
    "    # Store the numerical columns in a list numerical\n",
    "    numerical = data.columns.difference(categorical)   \n",
    "\n",
    "    # We create the preprocessing pipelines for both numeric and categorical data.\n",
    "    numeric_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='median')),\n",
    "        ('scaler', StandardScaler())])\n",
    "\n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "        ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "    preprocess = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', numeric_transformer, numerical),\n",
    "            ('cat', categorical_transformer, categorical)])\n",
    "    \n",
    "    pipeline = make_pipeline(preprocess)\n",
    "\n",
    "    # Split data into train and test sets\n",
    "    x_train, x_test, y_train, y_test = train_test_split(data, \n",
    "                                                        target, \n",
    "                                                        test_size=0.2,\n",
    "                                                        random_state=0,\n",
    "                                                        stratify=target)\n",
    "    \n",
    "    return x_train, x_test, y_train, y_test, pipeline, preprocess\n",
    "    \n",
    "# Load and preprocess data\n",
    "attrition_data = pd.read_csv('./data/data.csv')\n",
    "x_train, x_test, y_train, y_test, pipeline, preprocess = preprocess_data(attrition_data)\n",
    "\n",
    "# Transform data\n",
    "x_train_t = pipeline.fit_transform(x_train)\n",
    "x_test_t = pipeline.transform(x_test)\n",
    "\n",
    "# Create model\n",
    "model = tf.keras.models.Sequential()\n",
    "model.add(tf.keras.layers.Dense(units=16, activation='relu', input_shape=(x_train_t.shape[1],)))\n",
    "model.add(tf.keras.layers.Dense(units=16, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))\n",
    "\n",
    "# Compile model\n",
    "model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy']) \n",
    "\n",
    "# Fit model\n",
    "model.fit(x_train_t, y_train, epochs=20, verbose=1, batch_size=128, validation_data=(x_test_t, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explain Model Locally\n",
    "\n",
    "We will start by explaining the trained model locally."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Instantiate the explainer object using trained model.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from interpret.ext.greybox import DeepExplainer\n",
    "\n",
    "# explainer = DeepExplainer(model,\n",
    "#                           x_train,\n",
    "#                           features=x_train.columns,\n",
    "#                           classes=[\"STAYING\", \"LEAVING\"], \n",
    "#                           transformations = preprocess,\n",
    "#                           model_task=\"classification\",\n",
    "#                           is_classifier=True)\n",
    "\n",
    "import shap\n",
    "\n",
    "explainer = shap.DeepExplainer(model, x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init the JS visualization code\n",
    "shap.initjs()\n",
    "\n",
    "# transform the indexes to words\n",
    "import numpy as np\n",
    "words = imdb.get_word_index()\n",
    "num2word = {}\n",
    "for w in words.keys():\n",
    "    num2word[words[w]] = w\n",
    "x_test_words = np.stack([np.array(list(map(lambda x: num2word.get(x, \"NONE\"), x_test[i]))) for i in range(10)])\n",
    "\n",
    "# plot the explanation of the first prediction\n",
    "# Note the model is \"multi-output\" because it is rank-2 but only has one column\n",
    "shap.force_plot(explainer.expected_value[0], shap_values[0][0], x_test_words[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masker = shap.maskers.Text(tokenizer, mask_token = \"...\", collapse_mask_token=True)\n",
    "explainer = shap.Explainer(model, masker)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Generate global explanations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Passing in test dataset for evaluation examples - note it must be a representative sample of the original data\n",
    "# x_train can be passed as well, but with more examples explanations will take longer although they may be more accurate\n",
    "global_explanation = explainer.explain_global(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out a dictionary that holds the sorted feature importance names and values\n",
    "print('global importance rank: {}'.format(global_explanation.get_feature_importance_dict()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per class feature names\n",
    "print('ranked per class feature names: {}'.format(global_explanation.get_ranked_per_class_names()))\n",
    "\n",
    "# Per class feature importance values\n",
    "print('ranked per class feature values: {}'.format(global_explanation.get_ranked_per_class_values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Generate local explanations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can pass a specific data point or a group of data points to the explain_local function\n",
    "# E.g., Explain the first data point in the test set\n",
    "instance_num = 1\n",
    "local_explanation = explainer.explain_local(x_test[:instance_num])\n",
    "\n",
    "sorted_local_importance_values = local_explanation.get_ranked_local_values()\n",
    "sorted_local_importance_names = local_explanation.get_ranked_local_names()\n",
    "\n",
    "print('local importance values: {}'.format(sorted_local_importance_values))\n",
    "print('local importance names: {}'.format(sorted_local_importance_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Visualize our explanations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from interpret_community.widget import ExplanationDashboard\n",
    "from interpret_community.common.model_wrapper import wrap_model\n",
    "from interpret_community.dataset.dataset_wrapper import DatasetWrapper\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "wrapped_model, ml_domain = wrap_model(model, DatasetWrapper(x_test_t), \"classification\")\n",
    "wrapped_model.fit = model.fit\n",
    "dashboard_pipeline = Pipeline(steps=[('preprocess', preprocess), ('network', wrapped_model)])\n",
    "ExplanationDashboard(global_explanation, dashboard_pipeline, datasetX=x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
