{"nbformat_minor": 1, "nbformat": 4, "cells": [{"execution_count": null, "cell_type": "code", "source": ["'''\n", "STEP 1: Download Stack Overflow data from archive. (This takes about 2-3 hours)\n", "'''"], "outputs": [], "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": ["%sh \n", "wget https://archive.org/download/stackexchange/stackoverflow.com-Posts.7z\n", "sudo apt-get install p7zip-full\n", "7z x stackoverflow.com-Posts.7z -oposts"], "outputs": [], "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": ["'''\n", "STEP 2: Copy data over to databricks file system (This takes about an hour)\n", "'''"], "outputs": [], "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": ["dbutils.fs.cp(\"file:/databricks/driver/posts/Posts.xml\", \"dbfs:/tmp/posts/Posts.xml\")  "], "outputs": [], "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": ["'''\n", "STEP 3: Process data using Spark\n", "\n", "Note - This requires the spark-xml maven library (com.databricks:spark-xml_2.11:0.6.0) to be installed.\n", "'''\n", "import pyspark\n", "from pyspark.sql import functions as sf\n", "from pyspark.sql import Row\n", "from pyspark.sql.functions import size, col, concat_ws, rtrim, regexp_replace, split, udf\n", "from pyspark.sql.types import ArrayType\n", "\n", "# load xml file into spark data frame.\n", "posts = spark.read.format(\"xml\").option(\"rowTag\", \"row\").load(\"dbfs:/tmp/posts/Posts.xml\")\n", "\n", "# select only questions\n", "questions = posts.filter(posts._PostTypeId == 1) \n", "\n", "# drop irrelvant columns and clean up strings\n", "questions = questions.select([c for c in questions.columns if c in ['_Id','_Title','_Body','_Tags']])\n", "questions = questions.withColumn('full_question', sf.concat(sf.col('_Title'), sf.lit(' '), sf.col('_Body')))\n", "questions = questions.select([c for c in questions.columns if c in ['_Id','full_question','_Tags']]).withColumn(\"full_question\", regexp_replace(\"full_question\", \"[\\\\n,]\", \" \"))\n", "questions = questions.withColumn(\"_Tags\", regexp_replace(\"_Tags\", \"><\", \" \"))\n", "questions = questions.withColumn(\"_Tags\", regexp_replace(\"_Tags\", \"(>|<)\", \"\"))\n", "questions = questions.withColumn('_Tags', rtrim(questions._Tags))\n", "questions = questions.withColumn('_Tags', split(questions._Tags, \" \"))\n", "\n", "# filter out to single tags in following list\n", "tags_of_interest = ['azure-devops', 'azure-functions', 'azure-web-app-service', 'azure-storage', 'azure-virtual-machine'] \n", "\n", "def intersect(xs):\n", "    xs = set(xs)\n", "    @udf(\"array<string>\")\n", "    def _(ys):\n", "        return list(xs.intersection(ys))\n", "    return _\n", "\n", "questions = questions.withColumn(\"intersect\", intersect(tags_of_interest)(\"_Tags\"))\n", "questions = questions.filter(size(col(\"intersect\"))==1)\n", "questions = questions.select('_Id', 'full_question', 'intersect').withColumn('_Tags', concat_ws(', ', 'intersect'))\n", "questions = questions.select('_Id', 'full_question', '_Tags')\n", "\n", "questions.show()"], "outputs": [], "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": ["'''\n", "Step 4: Convert processed data into pandas data frame for final preprocessing and data split\n", "'''\n", "\n", "import pandas as pd\n", "import numpy as np\n", "from sklearn.utils import shuffle\n", "\n", "df = questions.toPandas()\n", "\n", "# drop nan values and remove line breaks\n", "df.dropna(inplace=True)\n", "df.replace(to_replace=[r\"\\\\t|\\\\n|\\\\r\", \"\\t|\\n|\\r\"], value=[\"\",\"\"], regex=True, inplace=True)\n", "\n", "# balance dataset \n", "balanced = df.groupby('_Tags')\n", "balanced.apply(lambda x: x.sample(balanced.size().min())).reset_index(drop=True).to_csv('balanced.csv')\n", "bd = pd.read_csv('balanced.csv')\n", "bd.drop('Unnamed: 0', axis=1, inplace=True)\n", "\n", "# shuffle data \n", "bd = shuffle(bd)\n", "\n", "# split data into train, test, and valid sets\n", "msk = np.random.rand(len(bd)) < 0.7\n", "train = bd[msk]\n", "temp = bd[~msk]\n", "msk = np.random.rand(len(temp)) < 0.66\n", "valid = temp[msk]\n", "test = temp[~msk]"], "outputs": [], "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": ["'''\n", "STEP 5: Save dataset into csv and class.txt files\n", "'''\n", "\n", "output_dir = './output'\n", "\n", "import os\n", "if not os.path.exists(output_dir):\n", "    os.makedirs(output_dir)\n", "\n", "# create and save classes.txt file\n", "classes = pd.DataFrame(bd['_Tags'].unique().tolist())\n", "classes.to_csv(os.path.join(output_dir, 'classes.txt'), header=False, index=False)\n", "\n", "# save train, valid, and test files\n", "train.to_csv(os.path.join(output_dir, 'train.csv'), header=False, index=False)\n", "valid.to_csv(os.path.join(output_dir, 'valid.csv'), header=False, index=False)\n", "test.to_csv(os.path.join(output_dir, 'test.csv'), header=False, index=False)"], "outputs": [], "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": ["'''\n", "STEP 5: Upload data to the default blob storage in your workspace.\n", "\n", "Note - This requires the azureml-sdk pip package to be installed.\n", "'''"], "outputs": [], "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": ["from azureml.core import Workspace\n", "from azureml.core import Datastore\n", "\n", "subscription_id='<SUBSCRIPTION-ID>'\n", "resource_group='<RESOURCE-GROUP>'\n", "workspace_name='<WORKSPACE-NAME>'\n", "\n", "ws = Workspace(subscription_id=subscription_id, \n", "               resource_group=resource_group, \n", "               workspace_name=workspace_name)\n", "\n", "ds = Datastore.get_default(ws)\n", "ds.upload(src_dir='./output', target_path='test-service-classifier/data')"], "outputs": [], "metadata": {}}], "metadata": {"kernelspec": {"display_name": "Python (fastai)", "name": "fastai", "language": "python"}, "language_info": {"mimetype": "text/x-python", "nbconvert_exporter": "python", "name": "python", "file_extension": ".py", "version": "3.7.3", "pygments_lexer": "ipython3", "codemirror_mode": {"version": 3, "name": "ipython"}}, "name": "stackoverflow-data-prep", "notebookId": 4233017539218929}}